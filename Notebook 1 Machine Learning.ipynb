{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147c05a6-3f9b-4ec8-996f-edf5ae23bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35e8f7e-540c-479d-8ed4-fd62d9e8c15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('train.csv')\n",
    "test_csv = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e5598-32e8-4a19-b59c-a0ed7bd299fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f38833d-0e9a-496a-9998-846ef4c23ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7140066c-7cb4-4ed3-b8c3-07b19d00c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv.shape, test_csv.shape, train_csv.isnull().values.any(), test_csv.isnull().values.any(), train_csv.duplicated().sum(), test_csv.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a10c8e-9243-4b2d-9233-7a0539f10d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv.Response.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f192ca02-0019-44fc-aa2f-1338b5eb321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "train_csv.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b426f-073e-480e-9b36-f49d5f1b0bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_info(df):\n",
    "    feature_info = {\n",
    "        'feature': df.columns,\n",
    "        'dtype': [df[col].dtype for col in df.columns],\n",
    "        'unique_val': [df[col].nunique() for col in df.columns]}\n",
    "    features_df = pd.DataFrame(feature_info)\n",
    "    features_df.set_index('feature', inplace=True)\n",
    "    features_df.sort_values(by='unique_val', ascending=False, inplace=True)\n",
    "    return features_df\n",
    "\n",
    "features_info(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc15fc59-4745-4b69-8897-7f3f438aaeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv.Region_Code.value_counts().tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cff869-53b2-4b72-8efa-413d72ef29fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = train_csv.drop(index=train_csv[train_csv.Region_Code==39.2].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1797efb-2234-47ac-98ab-2a5126714286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(raw_data):\n",
    "    raw_data.set_index('id', inplace=True)\n",
    "    raw_data.drop(columns=['Driving_License'], inplace=True)\n",
    "    raw_data.Previously_Insured = raw_data.Previously_Insured.astype(str)    \n",
    "    return raw_data\n",
    "\n",
    "data_clean(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec0641c-d3b0-4e84-a494-4220de810c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(10, 3), sharey=True)\n",
    "\n",
    "for ax, feature in zip(axes, train_csv.select_dtypes(include='object').columns):\n",
    "    sns.countplot(x=feature, data=train_csv, ax=ax, order=train_csv[feature].value_counts().index)\n",
    "    total = len(train_csv[feature])\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x() + p.get_width() / 2., height + 0.1, f'{height/total:.2%}', ha='center')\n",
    "\n",
    "for ax, feature in zip(axes, train_csv.select_dtypes(include='object').columns):\n",
    "    ax.set_title(f'{feature}', fontsize=12)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d04fff-49ff-4374-9a68-637bf3e6259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = train_csv[train_csv.Response==1]\n",
    "train_0 = train_csv[train_csv.Response==0].sample(train_1.shape[0])\n",
    "df_train = pd.concat([train_0, train_1])\n",
    "print(df_train.shape)\n",
    "df_train.Response.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb5698-87ae-4d90-b2e9-6c3de3822e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import mannwhitneyu, chi2_contingency\n",
    "\n",
    "y = df_train['Response']\n",
    "X = df_train.drop('Response', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=df_train['Response'])\n",
    "df_train = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded7725-9d80-4a10-be76-f5c9a681d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_info(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0e5634-a6a6-4ee8-a716-cf48a2a4444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "continous_features = X_train.select_dtypes(include='float64').drop(columns=['Policy_Sales_Channel', 'Region_Code']).columns\n",
    "nominal_features = X_train.select_dtypes(include='object').columns\n",
    "ordinal_features =  X_train.select_dtypes(exclude='object').drop(columns=['Annual_Premium', 'Region_Code']).columns\n",
    "target_encoded_feature = X_train.select_dtypes(include='float64').drop(columns=['Annual_Premium', 'Policy_Sales_Channel']).columns\n",
    "\n",
    "print(f\"continous_features: {continous_features}\")\n",
    "print(f\"nominal_features: {nominal_features}\")\n",
    "print(f\"ordinal_features: {ordinal_features}\")\n",
    "print(f\"target_encoded_feature: {target_encoded_feature}\")\n",
    "\n",
    "len(continous_features)+len(target_encoded_feature)+len(nominal_features)+len(ordinal_features), X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecde90e-728f-4198-8892-d6dd1f4208c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = continous_features.append(ordinal_features)\n",
    "\n",
    "for feature in numeric_features:\n",
    "    group_0 = df_train[df_train['Response']==0][feature]\n",
    "    group_1 = df_train[df_train['Response']==1][feature]\n",
    "    stat, p = mannwhitneyu(group_0, group_1, alternative='two-sided')\n",
    "    if p < 0.05:\n",
    "        print('*Feature {} is statistically significant'.format(feature))\n",
    "\n",
    "for feature in nominal_features:\n",
    "    contingency_table = pd.crosstab(df_train[feature], df_train['Response'])\n",
    "    chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)\n",
    "    if p < 0.05:\n",
    "        print('+Feature {} is statistically significant'.format(feature))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb307b16-7524-44a1-9927-c872d7bcf00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df_train.sample(n=8800) \n",
    "df_train = df_train.drop(sample.index)\n",
    "print(sample.shape)\n",
    "sample.Response.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2ecbb-8cb8-4fcf-8bae-17fde94d95d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, TargetEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "prep = make_column_transformer((StandardScaler(), continous_features),\n",
    "                               (TargetEncoder(), target_encoded_feature),\n",
    "                               (OneHotEncoder(drop='first'), nominal_features),\n",
    "                               (MinMaxScaler(), ordinal_features))\n",
    "pipelines = {\n",
    "    'logreg': make_pipeline(prep, LogisticRegression()),\n",
    "    'forest': make_pipeline(prep, RandomForestClassifier(LogisticRegression())),\n",
    "    'knn': make_pipeline(prep, KNeighborsClassifier())}\n",
    "\n",
    "hypergrid = {\n",
    "    'logreg': {\n",
    "        'logisticregression__C': [1.4, 2, 2.6, 4, 5.7, 7.8],\n",
    "        'logisticregression__solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    'forest': {\n",
    "        'randomforestclassifier__n_estimators': [72, 96, 124, 164, 221],\n",
    "        'randomforestclassifier__criterion': ['gini', 'entropy'],\n",
    "        'randomforestclassifier__min_samples_leaf': [12, 18, 36, 64, 88]\n",
    "    },\n",
    "    'knn': {\n",
    "        'kneighborsclassifier__n_neighbors': list(range(5, 51, 5)),\n",
    "        'kneighborsclassifier__weights': ['uniform', 'distance']\n",
    "    }}\n",
    "\n",
    "print('Training successfully Begun.\\n')\n",
    "\n",
    "models = {}\n",
    "for algo, pipeline in pipelines.items():\n",
    "    print('* Starting training for {}...'.format(algo))\n",
    "    start = time.time()\n",
    "    \n",
    "    model = GridSearchCV(pipeline, hypergrid[algo], cv=10, scoring='roc_auc')\n",
    "    model.fit(sample.drop('Response', axis=1), sample.Response)\n",
    "    models[algo] = model\n",
    "    \n",
    "    end = time.time()\n",
    "    print('  {} model fitted. ({:.2f} s)'.format(algo, end-start))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e377ca0c-2a17-4a00-9d94-bfcd3f26bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "y_test_summary = pd.DataFrame()\n",
    "\n",
    "for algo in pipelines.keys():\n",
    "    y_pred = models[algo].predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    TP = cm[1,1]\n",
    "    TN = cm[0,0]\n",
    "    FP = cm[0,1]\n",
    "    FN = cm[1,0]\n",
    "    y_test_summary.loc[algo,['Sensitivity']] = round(TP/float(TP + FN), 3)\n",
    "    y_test_summary.loc[algo,['Specificity']] = round(TN/float(TN + FP), 3)\n",
    "    y_pred = models[algo].predict_proba(X_test)[:, 1]\n",
    "    y_test_summary.loc[algo,['AUC']] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print('Predictions perspectives (sklearn algos):')\n",
    "y_test_summary.sort_values(by='AUC', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8483773f-25c9-401c-9b7d-5f6d11549391",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['forest'].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66351f0d-44b2-4947-8554-23d6e4590a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = models['forest'].best_params_\n",
    "\n",
    "prep = make_column_transformer((StandardScaler(), continous_features),\n",
    "                               (TargetEncoder(), target_encoded_feature),\n",
    "                               (OneHotEncoder(drop='first'), nominal_features),\n",
    "                               (MinMaxScaler(), ordinal_features))\n",
    "\n",
    "pipe = make_pipeline(prep, RandomForestClassifier(n_estimators = models['forest'].best_params_['randomforestclassifier__n_estimators'],\n",
    "                                                  criterion = models['forest'].best_params_['randomforestclassifier__criterion'],\n",
    "                                                  min_samples_leaf = models['forest'].best_params_['randomforestclassifier__min_samples_leaf']))\n",
    "\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30d879-1dd5-48f1-984f-2dbea0dcc2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pipe.predict(X_test))\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "\n",
    "Sensitivity = round(TP/float(TP + FN), 3)\n",
    "Specificity = round(TN/float(TN + FP), 3)\n",
    "Precision = round(TP/float(TP + FP), 3)\n",
    "print('Sensitivity: {}, Specificity: {}, Precision: {}'.format(Sensitivity, Specificity, Precision))\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "y_pred_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 4))\n",
    "\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "            yticklabels=['Actual 0', 'Actual 1'], cbar=False, ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix', fontsize=21)\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=16)\n",
    "axes[0].set_ylabel('True Label', fontsize=16)\n",
    "axes[0].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "\n",
    "axes[1].plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')\n",
    "axes[1].plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')  # Diagonal line\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('(1 - Specificity)', fontsize=18)\n",
    "axes[1].set_ylabel('Sensitivity', fontsize=18)\n",
    "axes[1].set_title('ROC Curve', fontsize=18)\n",
    "axes[1].legend(loc=\"lower right\", fontsize=14)\n",
    "\n",
    "default_threshold_index = np.where(thresholds > 0.5)[0][-1]\n",
    "axes[1].plot(fpr[default_threshold_index], tpr[default_threshold_index], 'ro', label='Threshold = 0.5')\n",
    "axes[1].legend(loc=\"lower right\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b00edaf-2c87-41c0-a482-974c8e072edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "y_hat = pipe.predict_proba(data_clean(test))[:, 1]\n",
    "submission = pd.DataFrame(test.reset_index().id).assign(Response=y_hat)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9e1ebf-3e7b-4986-93b2-84fda06e6c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the submission\n",
    "submission.to_csv('kaggle_submission.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
